{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ace0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parent_folder = (r'/Volumes/volume1/2025/D-Disease Modeling Team/MC-Temp/2D_ibidi_40x/DF10-11-12/MC25DF10-11-12-40x_Plate_22571/w3/')\n",
    "parent_folder = (r'/Users/lokesh.pimpale/Desktop/work_dir/sarcomere/test/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7520f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tifffile import imread, imwrite\n",
    "from sarcasm import *\n",
    "from sarcasm.export import Export\n",
    "import shutil\n",
    "import builtins\n",
    "\n",
    "# Save original print\n",
    "original_print = builtins.print  \n",
    "\n",
    "# Disable print\n",
    "builtins.print = lambda *a, **k: None  \n",
    "\n",
    "\n",
    "# --- set your input folder ---\n",
    "input_dir = Path(parent_folder)\n",
    "\n",
    "\n",
    "# --- loop over all tif/TIF files ---\n",
    "for f in sorted(input_dir.glob(\"*.tif\")) + sorted(input_dir.glob(\"*.TIF\")):\n",
    "    print(f\"Processing {f.name}...\")\n",
    "\n",
    "    # Load structure\n",
    "    sarc_obj = Structure(f,pixelsize=0.1699)\n",
    "    # detect sarcomere Z-bands, M-bands, orientation, sarcomere mask and cell mask by deep learning\n",
    "    sarc_obj.detect_sarcomeres(max_patch_size=(2048, 2048))\n",
    "    sarc_obj.analyze_cell_mask()\n",
    "    sarc_obj.analyze_z_bands()\n",
    "    sarc_obj.analyze_sarcomere_vectors()\n",
    "    sarc_obj.analyze_myofibrils()\n",
    "    sarc_obj.analyze_sarcomere_domains()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Get full dictionary of features\n",
    "    d = Export.get_structure_dict(sarc_obj)\n",
    "\n",
    "    # Keep only scalar entries (avoid ragged arrays)\n",
    "    clean = {}\n",
    "    for k, v in d.items():\n",
    "        try:\n",
    "            arr = np.asarray(v)\n",
    "            if arr.ndim == 0:  # scalar\n",
    "                clean[k] = arr.item()\n",
    "            elif arr.ndim == 1 and arr.size == 1:  # single-value array\n",
    "                clean[k] = arr[0].item()\n",
    "            # skip longer arrays\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    # Make DataFrame (one row of scalars)\n",
    "    df = pd.DataFrame([clean])\n",
    "\n",
    "    # Save as transposed table (key | value)\n",
    "    out_xlsx = f.parent / f\"{f.stem}_scalars.xlsx\"\n",
    "    df.T.to_excel(out_xlsx)\n",
    "\n",
    "    print(\"  → saved:\", out_xlsx)\n",
    "\n",
    "# Restore print\n",
    "builtins.print = original_print  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b64346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ============ USER EDITS ============\n",
    "input_dir = Path(parent_folder)\n",
    "# ====================================\n",
    "\n",
    "all_dfs = []\n",
    "\n",
    "for file in input_dir.glob(\"*.*\"):\n",
    "    if file.suffix.lower() not in [\".xls\", \".xlsx\"]:\n",
    "        continue\n",
    "    if \"_platemap\" in file.name:  # skip temp files\n",
    "        continue\n",
    "    try:\n",
    "        # read excel, skip first row\n",
    "        engine = \"openpyxl\" if file.suffix.lower() == \".xlsx\" else \"xlrd\"\n",
    "        df = pd.read_excel(file, header=None, skiprows=1, engine=engine)\n",
    "\n",
    "        # Expecting 2 columns: col0 = keys, col1 = values\n",
    "        df = df.iloc[:, :2]\n",
    "        df = df.dropna(how=\"all\")\n",
    "\n",
    "        # Transpose to a single row with keys as columns\n",
    "        df_t = df.set_index(0).T\n",
    "        df_t[\"filename\"] = file.stem\n",
    "\n",
    "        all_dfs.append(df_t)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Skipped {file.name}: {e}\")\n",
    "\n",
    "# Combine everything\n",
    "if all_dfs:\n",
    "    combined = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(combined.head())\n",
    "\n",
    "    # save to Excel\n",
    "    out_file = input_dir / \"combined_output.xlsx\"\n",
    "    combined.to_excel(out_file, index=False)\n",
    "    print(f\"✅ Saved combined dataframe to {out_file}\")\n",
    "else:\n",
    "    print(\"No valid .xls/.xlsx files processed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0690bb",
   "metadata": {},
   "source": [
    "prediction of good images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bf8172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re\n",
    "from pathlib import Path\n",
    "import pandas as pd \n",
    "from natsort import natsorted\n",
    "import seaborn as sns\n",
    "\n",
    "sarcomere_data = Path('/Volumes/volume2/2025/MC-Temporary/RvH-temp/Sarc_Examples')\n",
    "good_data = natsorted(list((sarcomere_data/'good').iterdir()))\n",
    "good_data = [x for x in good_data if 'DS_Store' not in x.name ]\n",
    "good_data = [x for x in good_data if 'xlsx'  in x.name ]\n",
    "good_sarc_features = pd.concat([pd.read_excel(file,index_col=0).T for file in good_data])\n",
    "good_sarc_features['class'] = 'good'\n",
    "good_sarc_features\n",
    "bad_data = natsorted(list((sarcomere_data/'bad').iterdir()))\n",
    "bad_data = [x for x in bad_data if 'DS_Store' not in x.name and r'~' not in x.name ]\n",
    "bad_data = [x for x in bad_data if 'xlsx'  in x.name ]\n",
    "bad_data\n",
    "\n",
    "\n",
    "bad_sarc_features = pd.DataFrame()\n",
    "\n",
    "for file in bad_data:\n",
    "    print(file)\n",
    "    f=  pd.read_excel(file,index_col=0).T\n",
    "    bad_sarc_features = pd.concat([bad_sarc_features,f])\n",
    "bad_sarc_features['class'] = 'bad'\n",
    "bad_sarc_features\n",
    "compare_df = pd.concat([good_sarc_features,bad_sarc_features])\n",
    "compare_df\n",
    "compare_df = compare_df.convert_dtypes()\n",
    "numeric_cols = list(compare_df.dtypes.index[(compare_df.dtypes == 'Float64') | (compare_df.dtypes == 'Int64')])\n",
    "numeric_cols\n",
    "subset_df = compare_df.loc[:,['class'] + numeric_cols]\n",
    "subset_df = subset_df.drop(columns=['frametime','timestamps','time','channel'])\n",
    "plotdf = subset_df.melt(id_vars = 'class')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "g = sns.FacetGrid(plotdf, col='variable',col_wrap=6,sharey=False)\n",
    "g.map_dataframe(sns.stripplot,x = 'class',y='value',hue='class',palette='RdBu')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from matplotlib.colors import ListedColormap\n",
    "def run_logistic_regression_analysis(df):\n",
    "    \"\"\"\n",
    "    This function takes a DataFrame, performs a logistic regression classification workflow\n",
    "    on unscaled data, including model training, weight inspection,\n",
    "    and visualization of the decision boundary. The DataFrame is expected to have\n",
    "    a 'class' column with 'good' and 'bad' values, and other numeric feature columns.\n",
    "    \"\"\"\n",
    "    # 1. --- Initial Data Inspection ---\n",
    "    print(\"--- Provided Data Head ---\")\n",
    "    print(df.head())\n",
    "    print(\"\\n--- Class Distribution ---\")\n",
    "    print(df['class'].value_counts())\n",
    "\n",
    "    # 2. --- Data Preparation ---\n",
    "    # Map target variable to numerical format\n",
    "    df['class'] = df['class'].map({'good': 1, 'bad': 0})\n",
    "\n",
    "    # Separate features (X) and target (y)\n",
    "    X = df.drop('class', axis=1)\n",
    "    y = df['class']\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    # 3. --- Build and Train the Model Pipeline ---\n",
    "    # Create a pipeline that first scales the data and then fits the logistic regression model.\n",
    "    # This is the correct way to handle scaling to prevent data leakage from the test set.\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logreg', LogisticRegression(random_state=42))\n",
    "    ])\n",
    "\n",
    "    # Train the pipeline on the training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # 4. --- Evaluate the Model ---\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    print(\"\\n--- Classification Report ---\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    print(\"\\n--- Confusion Matrix ---\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # 5. --- Display Model Weights (Coefficients) ---\n",
    "    # Access the logistic regression model and its coefficients from the pipeline\n",
    "    model = pipeline.named_steps['logreg']\n",
    "    coefficients = model.coef_[0]\n",
    "    intercept = model.intercept_[0]\n",
    "\n",
    "    # Create a DataFrame for better visualization of weights\n",
    "    weights = pd.DataFrame(\n",
    "        coefficients,\n",
    "        index=X.columns,\n",
    "        columns=['Weight']\n",
    "    ).sort_values('Weight', ascending=False)\n",
    "\n",
    "    print(\"\\n--- Model Weights ---\")\n",
    "    print(weights)\n",
    "    print(f\"\\nIntercept: {intercept:.4f}\")\n",
    "\n",
    "    # 6. --- Plot Decision Boundary ---\n",
    "    # For visualization, we can only plot in 2D. We will use the two features\n",
    "    # with the largest absolute weights.\n",
    "    most_important_features = weights.abs().sort_values('Weight', ascending=False).index[:2]\n",
    "    print(f\"\\nPlotting decision boundary for the two most important features: {most_important_features[0]} and {most_important_features[1]}\")\n",
    "\n",
    "    # Create a new pipeline with only the two selected features\n",
    "    X_vis = df[most_important_features]\n",
    "    y_vis = df['class']\n",
    "    \n",
    "    pipeline_vis = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('logreg', LogisticRegression(random_state=42))\n",
    "    ])\n",
    "    pipeline_vis.fit(X_vis, y_vis)\n",
    "\n",
    "    # Create a mesh grid\n",
    "    scaler_vis = pipeline_vis.named_steps['scaler']\n",
    "    X_scaled_vis = scaler_vis.transform(X_vis)\n",
    "\n",
    "    h = .02  # step size in the mesh\n",
    "    x_min, x_max = X_scaled_vis[:, 0].min() - .5, X_scaled_vis[:, 0].max() + .5\n",
    "    y_min, y_max = X_scaled_vis[:, 1].min() - .5, X_scaled_vis[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Predict on the mesh grid to get the decision boundary\n",
    "    Z = pipeline_vis.predict(scaler_vis.inverse_transform(np.c_[xx.ravel(), yy.ravel()]))\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cmap_light = ListedColormap(['#FFAAAA', '#AAAAFF']) # Colors for background\n",
    "    cmap_bold = ListedColormap(['#FF0000', '#0000FF'])   # Colors for points\n",
    "\n",
    "    plt.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.8)\n",
    "\n",
    "    # Plot the training points\n",
    "    plt.scatter(X_scaled_vis[:, 0], X_scaled_vis[:, 1], c=y_vis, cmap=cmap_bold,\n",
    "                edgecolor='k', s=50)\n",
    "    \n",
    "    plt.title(f'Logistic Regression Decision Boundary\\n(Features: {most_important_features[0]} & {most_important_features[1]})')\n",
    "    plt.xlabel(f'Scaled {most_important_features[0]}')\n",
    "    plt.ylabel(f'Scaled {most_important_features[1]}')\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.show()\n",
    "    \n",
    "    return pipeline\n",
    "train_df = subset_df.__deepcopy__()\n",
    "pipeline = run_logistic_regression_analysis(train_df)\n",
    "pipeline\n",
    "probs = pipeline.predict_proba(subset_df.iloc[:,1:])[:,1]\n",
    "predict_df = subset_df.__deepcopy__()\n",
    "predict_df['probs'] = probs\n",
    "\n",
    "sns.histplot(predict_df,x='probs',hue='class',bins=20,kde=True,palette='RdBu_r')\n",
    "\n",
    "model = pipeline.named_steps['logreg']\n",
    "weights = pd.DataFrame(\n",
    "        model.coef_[0],\n",
    "        index=subset_df.columns[1:],\n",
    "        columns=['Weight']\n",
    "    ).sort_values('Weight', ascending=False)\n",
    "weights= weights.reset_index(drop=False)\n",
    "weights = weights\n",
    "weights\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 8))\n",
    "sns.barplot(weights,x='Weight',y='index',hue='Weight',palette = 'RdBu_r')\n",
    "plt.axvline(x=0,linestyle='--',color='black')\n",
    "plt.title('Feature weights for good sarc images logreg pred',loc ='left')\n",
    "\n",
    "model.coef_[0][1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92328a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict_df = pd.read_excel('/Volumes/volume1/2025/D-Disease Modeling Team/MC-Temp/2D_ibidi_40x/DF10-11-12/MC25DF10-11-12-40x_Plate_22571/w3/combined_output.xlsx')\n",
    "predict_df = pd.read_excel(Path(parent_folder) / \"combined_output.xlsx\")\n",
    "def run_sarc_pred(sarc_features):\n",
    "    sarc_features =sarc_features.reset_index(drop=True)\n",
    "    sarc_features= sarc_features.loc[~pd.isna(sarc_features['domain_area_mean']),:]\n",
    "    sarc_features= sarc_features.loc[~pd.isna(sarc_features['z_intensity_mean']),:]\n",
    "    sarc_features = sarc_features.drop(columns=['frametime','timestamps','time','channel'])\n",
    "\n",
    "    predict_df = sarc_features.loc[:,[x for x in train_df.columns if 'class' not in x]]\n",
    "    sarc_features['class'] = pipeline.predict(predict_df)\n",
    "    return sarc_features\n",
    "predict_df = run_sarc_pred(predict_df)\n",
    "predict_df['Well'] = [x[-3] for x in predict_df['file_name'].str.split('_')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bea0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load your Excel file\n",
    "#platemap_df = pd.read_excel( \"/Volumes/volume1/2025/D-Disease Modeling Team/MC-Temp/2D_ibidi_40x/DF10-11-12/MC25DF10-11-12-40x_Plate_22571/w3/MCDF10-11-12_ibidi_HC_40x_platemap.xlsx\")\n",
    "platemap_files = list(parent_folder.glob(\"*_platemap*.xlsx\"))\n",
    "platemap_file = platemap_files[0]\n",
    "platemap_df = pd.read_excel(platemap_file)\n",
    "# ensure Well column is string\n",
    "platemap_df[\"Well\"] = platemap_df[\"Well\"].astype(str)\n",
    "\n",
    "# pad numeric part to 2 digits\n",
    "platemap_df[\"Well\"] = platemap_df[\"Well\"].str.replace(\n",
    "    r\"([A-Z])(\\d+)\", \n",
    "    lambda m: f\"{m.group(1)}{m.group(2).zfill(2)}\", \n",
    "    regex=True\n",
    ")\n",
    "\n",
    "#print(platemap_df[\"Well\"].head())\n",
    "\n",
    "#platemap_df\n",
    "predict_df =predict_df.merge(platemap_df,on='Well',how='left')\n",
    "for required_col in ['Condition','Treatment']:\n",
    "    if required_col not in predict_df.columns:\n",
    "        predict_df[required_col] = 'control'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b7736a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- SETTINGS ---\n",
    "value_col = \"sarcomere_length_mean\"   # metric to plot\n",
    "parent_folder = Path(parent_folder)   # ensure it's a Path, not str\n",
    "plot_dir = parent_folder / \"plots\"\n",
    "plot_dir.mkdir(parents=True, exist_ok=True)\n",
    "# ----------------\n",
    "\n",
    "# Drop placeholder rows\n",
    "platemap_df = platemap_df[platemap_df[\"Cond\"] != \"x\"].copy()\n",
    "\n",
    "# Merge on Well (already aligned + padded)\n",
    "merged = pd.merge(predict_df, platemap_df, on=\"Well\", how=\"inner\")\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot each BioRep separately\n",
    "for bio in merged[\"BioRep\"].dropna().unique():\n",
    "    df_bio = merged[merged[\"BioRep\"] == bio]\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.boxplot(data=df_bio, x=\"Cond\", y=value_col, hue=\"Treatment\")\n",
    "    plt.title(f\"{bio} - {value_col}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # save plot\n",
    "    bio_safe = str(bio).replace(\" \", \"_\")\n",
    "    out_file = plot_dir / f\"{bio_safe}_{value_col}.png\"\n",
    "    plt.savefig(out_file, dpi=300)\n",
    "    print(f\"✅ Saved {out_file}\")\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "# Combined plot (all BioReps together)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=merged, x=\"Cond\", y=value_col, hue=\"Treatment\")\n",
    "plt.title(f\"All BioReps - {value_col}\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "out_file = plot_dir / f\"All_BioReps_{value_col}.png\"\n",
    "plt.savefig(out_file, dpi=300)\n",
    "print(f\"✅ Saved {out_file}\")\n",
    "\n",
    "plt.close()\n",
    "\n",
    "# --- Export merged data to Excel ---\n",
    "excel_out = plot_dir / \"merged_data.xlsx\"\n",
    "merged.to_excel(excel_out, index=False)\n",
    "print(f\"✅ Exported merged data to {excel_out}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2d57b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc7b8506",
   "metadata": {},
   "source": [
    "### Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1942ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b177cb26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sarcasm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
