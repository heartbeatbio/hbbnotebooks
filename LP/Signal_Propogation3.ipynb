{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a9db59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d614748e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Found 2 grouped image sets.\n",
      "🧩 Combined stack shape: (3000, 1024, 1024)\n",
      "\n",
      "🔹 Analyzing RK25DE14B-D21_Stream_C07_s1_t1_FITC\n",
      "💾 Saved /Users/lokesh.pimpale/Desktop/work_dir/RK/untitled folder/RK25DE14B-D21_Stream_C07_s1_t1_FITC/RK25DE14B-D21_Stream_C07_s1_t1_FITC_event1_summary.png\n",
      "💾 Saved /Users/lokesh.pimpale/Desktop/work_dir/RK/untitled folder/RK25DE14B-D21_Stream_C07_s1_t1_FITC/RK25DE14B-D21_Stream_C07_s1_t1_FITC_event2_summary.png\n",
      "💾 Saved /Users/lokesh.pimpale/Desktop/work_dir/RK/untitled folder/RK25DE14B-D21_Stream_C07_s1_t1_FITC/RK25DE14B-D21_Stream_C07_s1_t1_FITC_event3_summary.png\n",
      "🧩 Combined stack shape: (3000, 1024, 1024)\n",
      "\n",
      "🔹 Analyzing RK25DE14B-D21_Stream_C08_s1_t1_FITC\n",
      "💾 Saved /Users/lokesh.pimpale/Desktop/work_dir/RK/untitled folder/RK25DE14B-D21_Stream_C08_s1_t1_FITC/RK25DE14B-D21_Stream_C08_s1_t1_FITC_event1_summary.png\n",
      "💾 Saved /Users/lokesh.pimpale/Desktop/work_dir/RK/untitled folder/RK25DE14B-D21_Stream_C08_s1_t1_FITC/RK25DE14B-D21_Stream_C08_s1_t1_FITC_event2_summary.png\n",
      "💾 Saved /Users/lokesh.pimpale/Desktop/work_dir/RK/untitled folder/RK25DE14B-D21_Stream_C08_s1_t1_FITC/RK25DE14B-D21_Stream_C08_s1_t1_FITC_event3_summary.png\n",
      "\n",
      "✅ Exported summary CSV: /Users/lokesh.pimpale/Desktop/work_dir/RK/untitled folder/CalciumEventSummary_LeadingEdges.csv\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tifffile as tiff\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage as ndi\n",
    "from scipy.signal import find_peaks, savgol_filter\n",
    "from skimage import measure, morphology, filters\n",
    "\n",
    "# =====================================================\n",
    "# CONFIGURATION\n",
    "# =====================================================\n",
    "input_dir = \"/Users/lokesh.pimpale/Desktop/work_dir/RK/untitled folder\"\n",
    "process_all = True\n",
    "single_tif_name = \"uniquename.tif\"\n",
    "\n",
    "frame_pad_left = 25\n",
    "frame_pad_right = 10\n",
    "std_thresh_factor = -0.5\n",
    "min_size_px = 500\n",
    "pixel_size_um = 3.14\n",
    "frame_rate = 50.0\n",
    "bin_size = 25\n",
    "threshold_frac = 0.75  # intensity fraction for edge detection\n",
    "edge_mode = \"image\"    # \"image\" (recommended) or \"map\"\n",
    "\n",
    "save_png = True\n",
    "show_plots = False\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# FILE GROUPING\n",
    "# =====================================================\n",
    "def group_tifs_by_basename(input_dir):\n",
    "    tif_files = sorted(\n",
    "        [os.path.join(input_dir, f) for f in os.listdir(input_dir)\n",
    "         if f.lower().endswith(\".tif\")]\n",
    "    )\n",
    "    groups = {}\n",
    "    for f in tif_files:\n",
    "        m = re.match(r\"^(.*?)(?:[-_]?file\\d{3})?\\.tif$\", os.path.basename(f), re.IGNORECASE)\n",
    "        if m:\n",
    "            base = os.path.join(input_dir, m.group(1))\n",
    "            groups.setdefault(base, []).append(f)\n",
    "    for base, files in groups.items():\n",
    "        files.sort(key=lambda x: int(re.search(r\"file(\\d{3})\", x).group(1))\n",
    "                   if re.search(r\"file(\\d{3})\", x) else 0)\n",
    "    print(f\"📂 Found {len(groups)} grouped image sets.\")\n",
    "    return groups\n",
    "\n",
    "\n",
    "def load_combined_stack(file_list):\n",
    "    parts = []\n",
    "    dtype = None\n",
    "    for f in file_list:\n",
    "        arr = tiff.imread(f)\n",
    "        if arr.ndim == 2:\n",
    "            arr = arr[np.newaxis, ...]\n",
    "        parts.append(arr)\n",
    "        if dtype is None:\n",
    "            dtype = arr.dtype\n",
    "    stack = np.concatenate(parts, axis=0).astype(dtype)\n",
    "    print(f\"🧩 Combined stack shape: {stack.shape}\")\n",
    "    return stack\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# HELPER FUNCTIONS\n",
    "# =====================================================\n",
    "def normalize01(a):\n",
    "    a = a.astype(np.float32)\n",
    "    mn, mx = np.nanmin(a), np.nanmax(a)\n",
    "    return (a - mn) / (mx - mn + 1e-6)\n",
    "\n",
    "\n",
    "def detect_events(data, smooth_win=15, prominence_factor=1.5, min_distance=5, baseline_drop=0.75):\n",
    "    mean_trace = np.mean(data.reshape(data.shape[0], -1), axis=1)\n",
    "    win = max(5, min(smooth_win, len(mean_trace)//2*2+1))\n",
    "    smoothed = savgol_filter(mean_trace, window_length=win, polyorder=3)\n",
    "    std = np.std(smoothed)\n",
    "    peaks, _ = find_peaks(smoothed, prominence=prominence_factor * std, distance=min_distance)\n",
    "\n",
    "    filtered = []\n",
    "    if len(peaks):\n",
    "        filtered.append(peaks[0])\n",
    "        for i in range(1, len(peaks)):\n",
    "            prev, cur = filtered[-1], peaks[i]\n",
    "            valley = np.min(smoothed[prev:cur])\n",
    "            thr = baseline_drop * min(smoothed[prev], smoothed[cur])\n",
    "            if valley < thr:\n",
    "                filtered.append(cur)\n",
    "\n",
    "    d1 = np.gradient(smoothed)\n",
    "    eps = 0.1 * np.std(d1)\n",
    "    landmarks = []\n",
    "    for p in filtered:\n",
    "        left = None\n",
    "        for i in range(p-1, 0, -1):\n",
    "            if np.all(d1[i:i+3] > eps):\n",
    "                left = i\n",
    "                break\n",
    "        if left is None:\n",
    "            left = max(0, p-10)\n",
    "        landmarks.append({\"left\": int(left), \"peak\": int(p)})\n",
    "    return landmarks, mean_trace, smoothed, np.array(filtered, int)\n",
    "\n",
    "\n",
    "def get_global_mask(data, min_size_px=500, relax=0.8):\n",
    "    mip = np.max(data, axis=0)\n",
    "    mip_norm = normalize01(mip)\n",
    "    thr = filters.threshold_otsu(mip_norm) * relax\n",
    "    binary = mip_norm > thr\n",
    "    filled = ndi.binary_fill_holes(binary)\n",
    "    cleaned = morphology.remove_small_objects(filled, min_size=min_size_px)\n",
    "    labels = measure.label(cleaned)\n",
    "    props = measure.regionprops(labels)\n",
    "    if not props:\n",
    "        return mip_norm, np.zeros_like(mip_norm, bool)\n",
    "    largest = max(props, key=lambda r: r.area).label\n",
    "    mask = ndi.binary_fill_holes(labels == largest)\n",
    "    return mip_norm, mask\n",
    "\n",
    "\n",
    "def analyze_event_std(event_data, global_mask, std_thresh_factor=0.0):\n",
    "    pixel_std = np.std(event_data, axis=0)\n",
    "    pixel_min = np.min(event_data, axis=0)\n",
    "    pixel_max = np.max(event_data, axis=0)\n",
    "    masked_std = pixel_std[global_mask]\n",
    "    mean_std, std_std = np.mean(masked_std), np.std(masked_std)\n",
    "    thr = mean_std + std_thresh_factor * std_std\n",
    "    dyn_mask = global_mask & (pixel_std >= thr)\n",
    "\n",
    "    frame_map = np.full_like(pixel_max, np.nan, np.float32)\n",
    "    level = pixel_min + 0.75 * (pixel_max - pixel_min)\n",
    "    for f in range(event_data.shape[0]):\n",
    "        hit = (event_data[f] >= level) & dyn_mask & np.isnan(frame_map)\n",
    "        frame_map[hit] = f\n",
    "    return pixel_std, dyn_mask, frame_map, thr\n",
    "\n",
    "\n",
    "def compute_leading_edges_from_image(event_data, dyn_mask, threshold_frac=0.75):\n",
    "    n_frames, H, W = event_data.shape\n",
    "    edges = []\n",
    "    pixel_min = np.min(event_data, axis=0)\n",
    "    pixel_max = np.max(event_data, axis=0)\n",
    "    level = pixel_min + threshold_frac * (pixel_max - pixel_min)\n",
    "    for f in range(n_frames):\n",
    "        active = (event_data[f] >= level) & dyn_mask\n",
    "        edge = active ^ ndi.binary_erosion(active)\n",
    "        edges.append(edge)\n",
    "    return edges\n",
    "\n",
    "\n",
    "def compute_leading_edges_from_map(frame_map, dyn_mask):\n",
    "    fmax = int(np.nanmax(frame_map))\n",
    "    edges = []\n",
    "    for f in range(fmax + 1):\n",
    "        active = (frame_map <= f) & dyn_mask\n",
    "        edge = active ^ ndi.binary_erosion(active)\n",
    "        edges.append(edge)\n",
    "    return edges\n",
    "\n",
    "\n",
    "def leading_edge_centroids(edges):\n",
    "    cents = []\n",
    "    for e in edges:\n",
    "        ysx = np.argwhere(e)\n",
    "        if ysx.size == 0:\n",
    "            cents.append((np.nan, np.nan))\n",
    "        else:\n",
    "            y, x = ysx.mean(axis=0)\n",
    "            cents.append((x, y))\n",
    "    return np.array(cents)\n",
    "\n",
    "\n",
    "def compute_propagation_distance(frame_map, dyn_mask, pixel_size_um, frame_rate, bin_size):\n",
    "    H, W = frame_map.shape\n",
    "    nY, nX = H // bin_size, W // bin_size\n",
    "    centers, times = [], []\n",
    "    for by in range(nY):\n",
    "        for bx in range(nX):\n",
    "            y0, y1, x0, x1 = by*bin_size, (by+1)*bin_size, bx*bin_size, (bx+1)*bin_size\n",
    "            region = frame_map[y0:y1, x0:x1]\n",
    "            vals = region[dyn_mask[y0:y1, x0:x1]]\n",
    "            vals = vals[np.isfinite(vals)]\n",
    "            if len(vals) < 3:\n",
    "                continue\n",
    "            centers.append((x0+bin_size/2, y0+bin_size/2))\n",
    "            times.append(np.nanmean(vals))\n",
    "    if not centers:\n",
    "        return np.nan, np.nan, np.nan, None, None\n",
    "    centers, times = np.array(centers), np.array(times)\n",
    "    i_start, i_end = np.argmin(times), np.argmax(times)\n",
    "    (x1, y1), (x2, y2) = centers[i_start], centers[i_end]\n",
    "    f1, f2 = times[i_start], times[i_end]\n",
    "    dist_px = np.sqrt((x2-x1)**2 + (y2-y1)**2)\n",
    "    dist_um = dist_px * pixel_size_um\n",
    "    dt_s = (f2 - f1) / frame_rate\n",
    "    vel_um_s = dist_um / dt_s if dt_s > 0 else np.nan\n",
    "    return dist_um, dt_s, vel_um_s, (x1, y1), (x2, y2)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# MAIN ANALYSIS\n",
    "# =====================================================\n",
    "def analyze_tif_stack(stack, base_name, output_root):\n",
    "    print(f\"\\n🔹 Analyzing {base_name}\")\n",
    "    n_frames, H, W = stack.shape\n",
    "    out_dir = os.path.join(output_root, base_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    landmarks, mean_trace, smoothed, peaks = detect_events(stack)\n",
    "    if not landmarks:\n",
    "        print(\"⚠️ No events detected.\")\n",
    "        return []\n",
    "\n",
    "    mip_norm, global_mask = get_global_mask(stack, min_size_px=min_size_px)\n",
    "\n",
    "    results = []\n",
    "    for i, ev in enumerate(landmarks, 1):\n",
    "        start = max(ev[\"left\"] - frame_pad_left, 0)\n",
    "        end = min(ev[\"peak\"] + frame_pad_right, n_frames)\n",
    "        event = stack[start:end]\n",
    "\n",
    "        pixel_std, dyn_mask, frame_map, thr = analyze_event_std(event, global_mask, std_thresh_factor)\n",
    "        edges = compute_leading_edges_from_image(event, dyn_mask) if edge_mode == \"image\" else compute_leading_edges_from_map(frame_map, dyn_mask)\n",
    "        cents = leading_edge_centroids(edges)\n",
    "\n",
    "        # compute propagation\n",
    "        dist_um, dt_s, vel_um_s, p_start, p_end = compute_propagation_distance(frame_map, dyn_mask, pixel_size_um, frame_rate, bin_size)\n",
    "\n",
    "        # save per-frame centroid CSV\n",
    "        df_event = pd.DataFrame({\n",
    "            \"frame_index\": np.arange(len(cents)),\n",
    "            \"centroid_x\": cents[:, 0],\n",
    "            \"centroid_y\": cents[:, 1],\n",
    "        })\n",
    "        if p_start and p_end:\n",
    "            df_event[\"start_x\"], df_event[\"start_y\"] = p_start\n",
    "            df_event[\"end_x\"], df_event[\"end_y\"] = p_end\n",
    "            df_event[\"distance_um\"] = dist_um\n",
    "        csv_path = os.path.join(out_dir, f\"{base_name}_event{i}_centroids.csv\")\n",
    "        df_event.to_csv(csv_path, index=False)\n",
    "\n",
    "        # === FIGURE ===\n",
    "        fig, axes = plt.subplots(1, 7, figsize=(36, 6))\n",
    "        axes[0].imshow(mip_norm, cmap=\"gray\"); axes[0].set_title(\"MIP\"); axes[0].axis(\"off\")\n",
    "        axes[1].imshow(pixel_std, cmap=\"magma\"); axes[1].imshow(global_mask, cmap=\"Greens\", alpha=0.3)\n",
    "        axes[1].set_title(\"STD + Global Mask\"); axes[1].axis(\"off\")\n",
    "        axes[2].imshow(dyn_mask, cmap=\"Blues\"); axes[2].set_title(\"Dynamic Mask\"); axes[2].axis(\"off\")\n",
    "        im3 = axes[3].imshow(frame_map + start, cmap=\"turbo\"); fig.colorbar(im3, ax=axes[3])\n",
    "        axes[3].set_title(\"Frame Map\"); axes[3].axis(\"off\")\n",
    "\n",
    "        # show edge traces on intensity\n",
    "        axes[4].imshow(np.mean(event, axis=0), cmap=\"gray\", alpha=0.6)\n",
    "        axes[4].set_title(\"Leading Edges (true from image)\"); axes[4].axis(\"off\")\n",
    "        idxs = np.linspace(0, len(edges)-1, 6, dtype=int)\n",
    "        cols = plt.cm.plasma(np.linspace(0, 1, len(idxs)))\n",
    "        for c, idx in zip(cols, idxs):\n",
    "            cnts = measure.find_contours(edges[idx].astype(float), 0.5)\n",
    "            for cnt in cnts:\n",
    "                axes[4].plot(cnt[:,1], cnt[:,0], color=c, lw=1.8)\n",
    "\n",
    "        # centroid trajectory\n",
    "        axes[5].imshow(np.mean(event, axis=0), cmap=\"gray\", alpha=0.6)\n",
    "        axes[5].set_title(\"Centroid Path\"); axes[5].axis(\"off\")\n",
    "        axes[5].plot(cents[:, 0], cents[:, 1], \"w--\", lw=2)\n",
    "        axes[5].scatter(cents[:, 0], cents[:, 1], s=40, c=np.linspace(0,1,len(cents)), cmap=\"cool\")\n",
    "        axes[5].scatter(cents[0, 0], cents[0, 1], s=100, c=\"lime\", edgecolors=\"k\")\n",
    "        axes[5].scatter(cents[-1, 0], cents[-1, 1], s=100, c=\"red\", edgecolors=\"k\")\n",
    "\n",
    "        # distance QC plot\n",
    "        axes[6].imshow(np.mean(event, axis=0), cmap=\"gray\", alpha=0.6)\n",
    "        axes[6].axis(\"off\")\n",
    "        if p_start and p_end:\n",
    "            axes[6].plot([p_start[0], p_end[0]], [p_start[1], p_end[1]], \"w--\", lw=2)\n",
    "            axes[6].scatter(*p_start, s=150, c=\"lime\", edgecolors=\"k\")\n",
    "            axes[6].scatter(*p_end, s=150, c=\"red\", edgecolors=\"k\")\n",
    "            axes[6].set_title(f\"Start-End Bins\\nDist={dist_um:.1f} µm\\nVel={vel_um_s:.1f} µm/s\")\n",
    "\n",
    "        plt.suptitle(f\"{base_name} — Event {i}\", fontsize=15)\n",
    "        plt.tight_layout()\n",
    "        out_path = os.path.join(out_dir, f\"{base_name}_event{i}_summary.png\")\n",
    "        plt.savefig(out_path, dpi=220)\n",
    "        plt.close(fig)\n",
    "        print(f\"💾 Saved {out_path}\")\n",
    "\n",
    "        results.append({\n",
    "            \"file\": base_name,\n",
    "            \"event_id\": i,\n",
    "            \"propagation_distance_um\": dist_um,\n",
    "            \"propagation_time_s\": dt_s,\n",
    "            \"propagation_velocity_um_s\": vel_um_s,\n",
    "            \"centroid_csv\": os.path.basename(csv_path)\n",
    "        })\n",
    "    return results\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# MAIN RUN\n",
    "# =====================================================\n",
    "if __name__ == \"__main__\":\n",
    "    all_results = []\n",
    "    file_groups = group_tifs_by_basename(input_dir)\n",
    "\n",
    "    if not process_all:\n",
    "        file_groups = {os.path.splitext(single_tif_name)[0]: [os.path.join(input_dir, single_tif_name)]}\n",
    "\n",
    "    for base, files in file_groups.items():\n",
    "        try:\n",
    "            stack = load_combined_stack(files)\n",
    "            all_results.extend(analyze_tif_stack(stack, os.path.basename(base), input_dir))\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error on {os.path.basename(base)}: {e}\")\n",
    "\n",
    "    if all_results:\n",
    "        df = pd.DataFrame(all_results)\n",
    "        csv_path = os.path.join(input_dir, \"CalciumEventSummary_LeadingEdges.csv\")\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        print(f\"\\n✅ Exported summary CSV: {csv_path}\")\n",
    "    else:\n",
    "        print(\"\\n⚠️ No events processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287228ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lokesh_3dseg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
